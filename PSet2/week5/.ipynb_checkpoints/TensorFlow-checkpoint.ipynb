{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# What's this TensorFlow business?\n",
    "\n",
    "You've written a lot of code in this assignment to provide a whole host of neural network functionality. Dropout, Batch Norm, and 2D convolutions are some of the workhorses of deep learning in computer vision. You've also worked hard to make your code efficient and vectorized.\n",
    "\n",
    "For the last part of this assignment, though, we're going to leave behind your beautiful codebase and instead migrate to one of two popular deep learning frameworks: in this instance, TensorFlow (or PyTorch, if you choose to work with that notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "#### What is it?\n",
    "TensorFlow is a system for executing computational graphs over Tensor objects, with native support for performing backpropogation for its Variables. In it, we work with Tensors which are n-dimensional arrays analogous to the numpy ndarray.\n",
    "\n",
    "#### Why?\n",
    "\n",
    "* Our code will now run on GPUs! Much faster training. Writing your own modules to run on GPUs is beyond the scope of this class, unfortunately.\n",
    "* We want you to be ready to use one of these frameworks for your project so you can experiment more efficiently than if you were writing every feature you want to use by hand. \n",
    "* We want you to stand on the shoulders of giants! TensorFlow and PyTorch are both excellent frameworks that will make your lives a lot easier, and now that you understand their guts, you are free to use them :) \n",
    "* We want you to be exposed to the sort of deep learning code you might run into in academia or industry. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acknowledgement: This exercise is adapted from [Stanford CS231n](http://cs231n.stanford.edu/index.html).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "## How will I learn TensorFlow?\n",
    "\n",
    "TensorFlow has many excellent tutorials available, including those from [Google themselves](https://www.tensorflow.org/get_started/get_started).\n",
    "\n",
    "Otherwise, this notebook will walk you through much of what you need to do to train models in TensorFlow. See the end of the notebook for some links to helpful tutorials if you want to learn more or need further clarification on topics that aren't fully explained here.\n",
    "\n",
    "**NOTE: This notebook is meant to teach you the latest version of Tensorflow 2.0. Most examples on the web today are still in 1.x, so be careful not to confuse the two when looking up documentation**.\n",
    "\n",
    "## Install Tensorflow 2.0\n",
    "Tensorflow 2.0 is still not in a fully 100% stable release, but it's still usable and more intuitive than TF 1.x. Please make sure you have it installed before moving on in this notebook! Here are some steps to get started:\n",
    "\n",
    "1. Have the latest version of Anaconda installed on your machine.\n",
    "2. Create a new conda environment starting from Python 3.7. In this setup example, we'll call it `tf_20_env`.\n",
    "3. Run the command: `source activate tf_20_env`\n",
    "4. Then pip install TF 2.0 as described here: https://www.tensorflow.org/install/pip \n",
    "\n",
    "A guide on creating Anaconda enviornments: https://uoa-eresearch.github.io/eresearch-cookbook/recipe/2014/11/20/conda/\n",
    "\n",
    "This will give you an new enviornemnt to play in TF 2.0. Generally, if you plan to also use TensorFlow in your other projects, you might also want to keep a seperate Conda environment or virtualenv in Python 3.7 that has Tensorflow 1.9, so you can switch back and forth at will. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acknowledgement: This exercise is adapted from [Stanford CS231n](http://cs231n.stanford.edu/index.html).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How will I learn TensorFlow?\n",
    "\n",
    "TensorFlow has many excellent tutorials available, including those from [Google themselves](https://www.tensorflow.org/get_started/get_started).\n",
    "\n",
    "Otherwise, this notebook will walk you through much of what you need to do to train models in TensorFlow. See the end of the notebook for some links to helpful tutorials if you want to learn more or need further clarification on topics that aren't fully explained here.\n",
    "\n",
    "# Part I: Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf version: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "print(\"tf version: \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 32, 32, 3)\n",
      "Train labels shape:  (49000,) int32\n",
      "Validation data shape:  (1000, 32, 32, 3)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "def load_cifar10(num_training=49000, num_validation=1000, num_test=10000):\n",
    "    \"\"\"\n",
    "    Fetch the CIFAR-10 dataset from the web and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.\n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 dataset and use appropriate data types and shapes\n",
    "    cifar10 = tf.keras.datasets.cifar10.load_data()\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10\n",
    "    X_train = np.asarray(X_train, dtype=np.float32)\n",
    "    y_train = np.asarray(y_train, dtype=np.int32).flatten()\n",
    "    X_test = np.asarray(X_test, dtype=np.float32)\n",
    "    y_test = np.asarray(y_test, dtype=np.int32).flatten()\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean pixel and divide by std\n",
    "    mean_pixel = X_train.mean(axis=(0, 1, 2), keepdims=True)\n",
    "    std_pixel = X_train.std(axis=(0, 1, 2), keepdims=True)\n",
    "    X_train = (X_train - mean_pixel) / std_pixel\n",
    "    X_val = (X_val - mean_pixel) / std_pixel\n",
    "    X_test = (X_test - mean_pixel) / std_pixel\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# If there are errors with SSL downloading involving self-signed certificates,\n",
    "# it may be that your Python version was recently installed on the current machine.\n",
    "# See: https://github.com/tensorflow/tensorflow/issues/10779\n",
    "# To fix, run the command: /Applications/Python\\ 3.7/Install\\ Certificates.command\n",
    "#   ...replacing paths as necessary.\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "NHW = (0, 1, 2)\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape, y_train.dtype)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, X, y, batch_size, shuffle=False):\n",
    "        \"\"\"\n",
    "        Construct a Dataset object to iterate over data X and labels y\n",
    "        \n",
    "        Inputs:\n",
    "        - X: Numpy array of data, of any shape\n",
    "        - y: Numpy array of labels, of any shape but with y.shape[0] == X.shape[0]\n",
    "        - batch_size: Integer giving number of elements per minibatch\n",
    "        - shuffle: (optional) Boolean, whether to shuffle the data on each epoch\n",
    "        \"\"\"\n",
    "        assert X.shape[0] == y.shape[0], 'Got different numbers of data and labels'\n",
    "        self.X, self.y = X, y\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        N, B = self.X.shape[0], self.batch_size\n",
    "        idxs = np.arange(N)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxs)\n",
    "        return iter((self.X[i:i+B], self.y[i:i+B]) for i in range(0, N, B))\n",
    "\n",
    "\n",
    "train_dset = Dataset(X_train, y_train, batch_size=64, shuffle=True)\n",
    "val_dset = Dataset(X_val, y_val, batch_size=64, shuffle=False)\n",
    "test_dset = Dataset(X_test, y_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (64, 32, 32, 3) (64,)\n",
      "1 (64, 32, 32, 3) (64,)\n",
      "2 (64, 32, 32, 3) (64,)\n",
      "3 (64, 32, 32, 3) (64,)\n",
      "4 (64, 32, 32, 3) (64,)\n",
      "5 (64, 32, 32, 3) (64,)\n",
      "6 (64, 32, 32, 3) (64,)\n"
     ]
    }
   ],
   "source": [
    "# We can iterate through a dataset like this:\n",
    "for t, (x, y) in enumerate(train_dset):\n",
    "    print(t, x.shape, y.shape)\n",
    "    if t > 5: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can optionally **use GPU by setting the flag to True below**. It's not neccessary to use a GPU for this assignment; if you are working on Google Cloud then we recommend that you do not use a GPU, as it will be significantly more expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  /cpu:0\n"
     ]
    }
   ],
   "source": [
    "# Set up some global variables\n",
    "USE_GPU = False\n",
    "\n",
    "if USE_GPU:\n",
    "    device = '/device:GPU:0'\n",
    "else:\n",
    "    device = '/cpu:0'\n",
    "\n",
    "# Constant to control how often we print when training models\n",
    "print_every = 100\n",
    "\n",
    "print('Using device: ', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "# Part II: Barebones TensorFlow\n",
    "TensorFlow ships with various high-level APIs which make it very convenient to define and train neural networks; we will cover some of these constructs in Part III and Part IV of this notebook. In this section we will start by building a model with basic TensorFlow constructs to help you better understand what's going on under the hood of the higher-level APIs.\n",
    "\n",
    "**\"Barebones Tensorflow\" is important to understanding the building blocks of TensorFlow, but much of it involves concepts from TensorFlow 1.x.** We will be working with legacy modules such as `tf.Variable`.\n",
    "\n",
    "Therefore, please read and understand the differences between legacy (1.x) TF and the new (2.0) TF.\n",
    "\n",
    "### Historical background on TensorFlow 1.x\n",
    "\n",
    "TensorFlow 1.x is primarily a framework for working with **static computational graphs**. Nodes in the computational graph are Tensors which will hold n-dimensional arrays when the graph is run; edges in the graph represent functions that will operate on Tensors when the graph is run to actually perform useful computation.\n",
    "\n",
    "Before Tensorflow 2.0, we had to configure the graph into two phases. There are plenty of tutorials online that explain this two-step process. The process generally looks like the following for TF 1.x:\n",
    "1. **Build a computational graph that describes the computation that you want to perform**. This stage doesn't actually perform any computation; it just builds up a symbolic representation of your computation. This stage will typically define one or more `placeholder` objects that represent inputs to the computational graph.\n",
    "2. **Run the computational graph many times.** Each time the graph is run (e.g. for one gradient descent step) you will specify which parts of the graph you want to compute, and pass a `feed_dict` dictionary that will give concrete values to any `placeholder`s in the graph.\n",
    "\n",
    "### The new paradigm in Tensorflow 2.0\n",
    "Now, with Tensorflow 2.0, we can simply adopt a functional form that is more Pythonic and similar in spirit to PyTorch and direct Numpy operation. Instead of the 2-step paradigm with computation graphs, making it (among other things) easier to debug TF code. You can read more details at https://www.tensorflow.org/guide/eager.\n",
    "\n",
    "The main difference between the TF 1.x and 2.0 approach is that the 2.0 approach doesn't make use of `tf.Session`, `tf.run`, `placeholder`, `feed_dict`. To get more details of what's different between the two version and how to convert between the two, check out the official migration guide: https://www.tensorflow.org/alpha/guide/migration_guide\n",
    "\n",
    "Later, in the rest of this notebook we'll focus on this new, simpler approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "source": [
    "### TensorFlow warmup: Flatten Function\n",
    "\n",
    "We can see this in action by defining a simple `flatten` function that will reshape image data for use in a fully-connected network.\n",
    "\n",
    "In TensorFlow, data for convolutional feature maps is typically stored in a Tensor of shape N x H x W x C where:\n",
    "\n",
    "- N is the number of datapoints (minibatch size)\n",
    "- H is the height of the feature map\n",
    "- W is the width of the feature map\n",
    "- C is the number of channels in the feature map\n",
    "\n",
    "This is the right way to represent the data when we are doing something like a 2D convolution, that needs spatial understanding of where the intermediate features are relative to each other. When we use fully connected affine layers to process the image, however, we want each datapoint to be represented by a single vector -- it's no longer useful to segregate the different channels, rows, and columns of the data. So, we use a \"flatten\" operation to collapse the `H x W x C` values per representation into a single long vector. \n",
    "\n",
    "Notice the `tf.reshape` call has the target shape as `(N, -1)`, meaning it will reshape/keep the first dimension to be N, and then infer as necessary what the second dimension is in the output, so we can collapse the remaining dimensions from the input properly.\n",
    "\n",
    "**NOTE**: TensorFlow and PyTorch differ on the default Tensor layout; TensorFlow uses N x H x W x C but PyTorch uses N x C x H x W."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    \"\"\"    \n",
    "    Input:\n",
    "    - TensorFlow Tensor of shape (N, D1, ..., DM)\n",
    "    \n",
    "    Output:\n",
    "    - TensorFlow Tensor of shape (N, D1 * ... * DM)\n",
    "    \"\"\"\n",
    "    N = tf.shape(x)[0]\n",
    "    return tf.reshape(x, (N, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_np:\n",
      " [[[ 0  1  2  3]\n",
      "  [ 4  5  6  7]\n",
      "  [ 8  9 10 11]]\n",
      "\n",
      " [[12 13 14 15]\n",
      "  [16 17 18 19]\n",
      "  [20 21 22 23]]] \n",
      "\n",
      "x_flat_np:\n",
      " tf.Tensor(\n",
      "[[ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
      " [12 13 14 15 16 17 18 19 20 21 22 23]], shape=(2, 12), dtype=int64) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_flatten():\n",
    "    # Construct concrete values of the input data x using numpy\n",
    "    x_np = np.arange(24).reshape((2, 3, 4))\n",
    "    print('x_np:\\n', x_np, '\\n')\n",
    "    # Compute a concrete output value.\n",
    "    x_flat_np = flatten(x_np)\n",
    "    print('x_flat_np:\\n', x_flat_np, '\\n')\n",
    "\n",
    "test_flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barebones TensorFlow: Define a Two-Layer Network\n",
    "We will now implement our first neural network with TensorFlow: a fully-connected ReLU network with two hidden layers and no biases on the CIFAR10 dataset. For now we will use only low-level TensorFlow operators to define the network; later we will see how to use the higher-level abstractions provided by `tf.keras` to simplify the process.\n",
    "\n",
    "We will define the forward pass of the network in the function `two_layer_fc`; this will accept TensorFlow Tensors for the inputs and weights of the network, and return a TensorFlow Tensor for the scores. \n",
    "\n",
    "After defining the network architecture in the `two_layer_fc` function, we will test the implementation by checking the shape of the output.\n",
    "\n",
    "**It's important that you read and understand this implementation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "def two_layer_fc(x, params):\n",
    "    \"\"\"\n",
    "    A fully-connected neural network; the architecture is:\n",
    "    fully-connected layer -> ReLU -> fully connected layer.\n",
    "    Note that we only need to define the forward pass here; TensorFlow will take\n",
    "    care of computing the gradients for us.\n",
    "    \n",
    "    The input to the network will be a minibatch of data, of shape\n",
    "    (N, d1, ..., dM) where d1 * ... * dM = D. The hidden layer will have H units,\n",
    "    and the output layer will produce scores for C classes.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A TensorFlow Tensor of shape (N, d1, ..., dM) giving a minibatch of\n",
    "      input data.\n",
    "    - params: A list [w1, w2] of TensorFlow Tensors giving weights for the\n",
    "      network, where w1 has shape (D, H) and w2 has shape (H, C).\n",
    "    \n",
    "    Returns:\n",
    "    - scores: A TensorFlow Tensor of shape (N, C) giving classification scores\n",
    "      for the input data x.\n",
    "    \"\"\"\n",
    "    w1, w2 = params                   # Unpack the parameters\n",
    "    x = flatten(x)                    # Flatten the input; now x has shape (N, D)\n",
    "    h = tf.nn.relu(tf.matmul(x, w1))  # Hidden layer: h has shape (N, H)\n",
    "    scores = tf.matmul(h, w2)         # Compute scores of shape (N, C)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n"
     ]
    }
   ],
   "source": [
    "def two_layer_fc_test():\n",
    "    hidden_layer_size = 42\n",
    "\n",
    "    # Scoping our TF operations under a tf.device context manager \n",
    "    # lets us tell TensorFlow where we want these Tensors to be\n",
    "    # multiplied and/or operated on, e.g. on a CPU or a GPU.\n",
    "    with tf.device(device):        \n",
    "        x = tf.zeros((64, 32, 32, 3))\n",
    "        w1 = tf.zeros((32 * 32 * 3, hidden_layer_size))\n",
    "        w2 = tf.zeros((hidden_layer_size, 10))\n",
    "\n",
    "        # Call our two_layer_fc function for the forward pass of the network.\n",
    "        scores = two_layer_fc(x, [w1, w2])\n",
    "\n",
    "    print(scores.shape)\n",
    "\n",
    "two_layer_fc_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barebones TensorFlow: Three-Layer ConvNet\n",
    "Here you will complete the implementation of the function `three_layer_convnet` which will perform the forward pass of a three-layer convolutional network. The network should have the following architecture:\n",
    "\n",
    "1. A convolutional layer (with bias) with `channel_1` filters, each with shape `KW1 x KH1`, and zero-padding of two\n",
    "2. ReLU nonlinearity\n",
    "3. A convolutional layer (with bias) with `channel_2` filters, each with shape `KW2 x KH2`, and zero-padding of one\n",
    "4. ReLU nonlinearity\n",
    "5. Fully-connected layer with bias, producing scores for `C` classes.\n",
    "\n",
    "**HINT**: For convolutions: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/nn/conv2d; be careful with padding!\n",
    "\n",
    "**HINT**: For biases: https://www.tensorflow.org/performance/xla/broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_layer_convnet(x, params):\n",
    "    \"\"\"\n",
    "    A three-layer convolutional network with the architecture described above.\n",
    "    \n",
    "    Inputs:\n",
    "    - x: A TensorFlow Tensor of shape (N, H, W, 3) giving a minibatch of images\n",
    "    - params: A list of TensorFlow Tensors giving the weights and biases for the\n",
    "      network; should contain the following:\n",
    "      - conv_w1: TensorFlow Tensor of shape (KH1, KW1, 3, channel_1) giving\n",
    "        weights for the first convolutional layer.\n",
    "      - conv_b1: TensorFlow Tensor of shape (channel_1,) giving biases for the\n",
    "        first convolutional layer.\n",
    "      - conv_w2: TensorFlow Tensor of shape (KH2, KW2, channel_1, channel_2)\n",
    "        giving weights for the second convolutional layer\n",
    "      - conv_b2: TensorFlow Tensor of shape (channel_2,) giving biases for the\n",
    "        second convolutional layer.\n",
    "      - fc_w: TensorFlow Tensor giving weights for the fully-connected layer.\n",
    "        Can you figure out what the shape should be?\n",
    "      - fc_b: TensorFlow Tensor giving biases for the fully-connected layer.\n",
    "        Can you figure out what the shape should be?\n",
    "    \"\"\"\n",
    "    conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b = params\n",
    "    scores = None\n",
    "    ############################################################################\n",
    "    # TODO: Implement the forward pass for the three-layer ConvNet.            #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    conv_1 = tf.math.add(tf.nn.conv2d(input=x, filters=conv_w1, strides=1, padding=\"SAME\", data_format='NHWC'), conv_b1)\n",
    "    relu_1 = tf.nn.relu(conv_1)\n",
    "    conv_2 = tf.math.add(tf.nn.conv2d(input=relu_1, filters=conv_w2, strides=1, padding=\"SAME\", data_format='NHWC'), conv_b2)\n",
    "    relu_2 = tf.nn.relu(conv_2)\n",
    "    scores = tf.math.add(tf.matmul(flatten(relu_2), fc_w), fc_b)\n",
    "    pass\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                              END OF YOUR CODE                            #\n",
    "    ############################################################################\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defing the forward pass of the three-layer ConvNet above, run the following cell to test your implementation. Like the two-layer network, we run the graph on a batch of zeros just to make sure the function doesn't crash, and produces outputs of the correct shape.\n",
    "\n",
    "When you run this function, `scores_np` should have shape `(64, 10)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "pdf-ignore-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores_np has shape:  (64, 10)\n"
     ]
    }
   ],
   "source": [
    "def three_layer_convnet_test():\n",
    "    \n",
    "    with tf.device(device):\n",
    "        x = tf.zeros((64, 32, 32, 3))\n",
    "        conv_w1 = tf.zeros((5, 5, 3, 6))\n",
    "        conv_b1 = tf.zeros((6,))\n",
    "        conv_w2 = tf.zeros((3, 3, 6, 9))\n",
    "        conv_b2 = tf.zeros((9,))\n",
    "        fc_w = tf.zeros((32 * 32 * 9, 10))\n",
    "        fc_b = tf.zeros((10,))\n",
    "        params = [conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b]\n",
    "        scores = three_layer_convnet(x, params)\n",
    "\n",
    "    # Inputs to convolutional layers are 4-dimensional arrays with shape\n",
    "    # [batch_size, height, width, channels]\n",
    "    print('scores_np has shape: ', scores.shape)\n",
    "\n",
    "three_layer_convnet_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barebones TensorFlow: Training Step\n",
    "\n",
    "We now define the `training_step` function performs a single training step. This will take three basic steps:\n",
    "\n",
    "1. Compute the loss\n",
    "2. Compute the gradient of the loss with respect to all network weights\n",
    "3. Make a weight update step using (stochastic) gradient descent.\n",
    "\n",
    "\n",
    "We need to use a few new TensorFlow functions to do all of this:\n",
    "- For computing the cross-entropy loss we'll use `tf.nn.sparse_softmax_cross_entropy_with_logits`: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits\n",
    "\n",
    "- For averaging the loss across a minibatch of data we'll use `tf.reduce_mean`:\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/reduce_mean\n",
    "\n",
    "- For computing gradients of the loss with respect to the weights we'll use `tf.GradientTape` (useful for Eager execution):  https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/GradientTape\n",
    "\n",
    "- We'll mutate the weight values stored in a TensorFlow Tensor using `tf.assign_sub` (\"sub\" is for subtraction): https://www.tensorflow.org/api_docs/python/tf/assign_sub \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "def training_step(model_fn, x, y, params, learning_rate):\n",
    "    with tf.GradientTape() as tape:\n",
    "        scores = model_fn(x, params) # Forward pass of the model\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=scores)\n",
    "        total_loss = tf.reduce_mean(loss)\n",
    "        grad_params = tape.gradient(total_loss, params)\n",
    "\n",
    "        # Make a vanilla gradient descent step on all of the model parameters\n",
    "        # Manually update the weights using assign_sub()\n",
    "        for w, grad_w in zip(params, grad_params):\n",
    "            w.assign_sub(learning_rate * grad_w)\n",
    "                        \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "def train_part2(model_fn, init_fn, learning_rate, epochs):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10.\n",
    "    \n",
    "    Inputs:\n",
    "    - model_fn: A Python function that performs the forward pass of the model\n",
    "      using TensorFlow; it should have the following signature:\n",
    "      scores = model_fn(x, params) where x is a TensorFlow Tensor giving a\n",
    "      minibatch of image data, params is a list of TensorFlow Tensors holding\n",
    "      the model weights, and scores is a TensorFlow Tensor of shape (N, C)\n",
    "      giving scores for all elements of x.\n",
    "    - init_fn: A Python function that initializes the parameters of the model.\n",
    "      It should have the signature params = init_fn() where params is a list\n",
    "      of TensorFlow Tensors holding the (randomly initialized) weights of the\n",
    "      model.\n",
    "    - learning_rate: Python float giving the learning rate to use for SGD.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    params = init_fn()  # Initialize the model parameters            \n",
    "    for e in range(epochs):    \n",
    "        for t, (x_np, y_np) in enumerate(train_dset):\n",
    "            # Run the graph on a batch of training data.\n",
    "            loss = training_step(model_fn, x_np, y_np, params, learning_rate)\n",
    "\n",
    "            # Periodically print the loss and check accuracy on the val set.\n",
    "            if t % print_every == 0:\n",
    "                print('Epoch %d, iteration %d, loss = %.4f' % (e, t, loss))\n",
    "                print('Validation:')\n",
    "                check_accuracy(val_dset, model_fn, params)\n",
    "    return params\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "def check_accuracy(dset, model_fn, params):\n",
    "    \"\"\"\n",
    "    Check accuracy on a classification model, e.g. for validation.\n",
    "    \n",
    "    Inputs:\n",
    "    - dset: A Dataset object against which to check accuracy\n",
    "    - x: A TensorFlow placeholder Tensor where input images should be fed\n",
    "    - model_fn: the Model we will be calling to make predictions on x\n",
    "    - params: parameters for the model_fn to work with\n",
    "      \n",
    "    Returns: Nothing, but prints the accuracy of the model\n",
    "    \"\"\"\n",
    "    num_correct, num_samples = 0, 0\n",
    "    for x_batch, y_batch in dset:\n",
    "        scores_np = model_fn(x_batch, params).numpy()\n",
    "        y_pred = scores_np.argmax(axis=1)\n",
    "        num_samples += x_batch.shape[0]\n",
    "        num_correct += (y_pred == y_batch).sum()\n",
    "    acc = float(num_correct) / num_samples\n",
    "    print('     Got %d / %d correct (%.2f%%)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barebones TensorFlow: Initialization\n",
    "We'll use the following utility method to initialize the weight matrices for our models using Kaiming's normalization method.\n",
    "\n",
    "[1] He et al, *Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\n",
    "*, ICCV 2015, https://arxiv.org/abs/1502.01852"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matrix_with_kaiming_normal(shape):\n",
    "    if len(shape) == 2:\n",
    "        fan_in, fan_out = shape[0], shape[1]\n",
    "    elif len(shape) == 4:\n",
    "        fan_in, fan_out = np.prod(shape[:3]), shape[3]\n",
    "    return tf.keras.backend.random_normal(shape) * np.sqrt(2.0 / fan_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barebones TensorFlow: Train a Two-Layer Network\n",
    "We are finally ready to use all of the pieces defined above to train a two-layer fully-connected network on CIFAR-10.\n",
    "\n",
    "We just need to define a function to initialize the weights of the model, and call `train_part2`.\n",
    "\n",
    "Defining the weights of the network introduces another important piece of TensorFlow API: `tf.Variable`. A TensorFlow Variable is a Tensor whose value is stored in the graph and persists across runs of the computational graph; however unlike constants defined with `tf.zeros` or `tf.random_normal`, the values of a Variable can be mutated as the graph runs; these mutations will persist across graph runs. Learnable parameters of the network are usually stored in Variables.\n",
    "\n",
    "You don't need to tune any hyperparameters, but you should achieve validation accuracies above 40% after one epoch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Epoch 0, iteration 0, loss = 3.1930\n",
      "Validation:\n",
      "     Got 119 / 1000 correct (11.90%)\n",
      "Epoch 0, iteration 100, loss = 1.8510\n",
      "Validation:\n",
      "     Got 380 / 1000 correct (38.00%)\n",
      "Epoch 0, iteration 200, loss = 1.4407\n",
      "Validation:\n",
      "     Got 392 / 1000 correct (39.20%)\n",
      "Epoch 0, iteration 300, loss = 1.8373\n",
      "Validation:\n",
      "     Got 381 / 1000 correct (38.10%)\n",
      "Epoch 0, iteration 400, loss = 1.8028\n",
      "Validation:\n",
      "     Got 413 / 1000 correct (41.30%)\n",
      "Epoch 0, iteration 500, loss = 1.7373\n",
      "Validation:\n",
      "     Got 437 / 1000 correct (43.70%)\n",
      "Epoch 0, iteration 600, loss = 1.9038\n",
      "Validation:\n",
      "     Got 411 / 1000 correct (41.10%)\n",
      "Epoch 0, iteration 700, loss = 2.0090\n",
      "Validation:\n",
      "     Got 436 / 1000 correct (43.60%)\n",
      "Epoch 1, iteration 0, loss = 1.4770\n",
      "Validation:\n",
      "     Got 435 / 1000 correct (43.50%)\n",
      "Epoch 1, iteration 100, loss = 1.5552\n",
      "Validation:\n",
      "     Got 489 / 1000 correct (48.90%)\n",
      "Epoch 1, iteration 200, loss = 1.2209\n",
      "Validation:\n",
      "     Got 481 / 1000 correct (48.10%)\n",
      "Epoch 1, iteration 300, loss = 1.5236\n",
      "Validation:\n",
      "     Got 437 / 1000 correct (43.70%)\n",
      "Epoch 1, iteration 400, loss = 1.4583\n",
      "Validation:\n",
      "     Got 466 / 1000 correct (46.60%)\n",
      "Epoch 1, iteration 500, loss = 1.5403\n",
      "Validation:\n",
      "     Got 476 / 1000 correct (47.60%)\n",
      "Epoch 1, iteration 600, loss = 1.6755\n",
      "Validation:\n",
      "     Got 464 / 1000 correct (46.40%)\n",
      "Epoch 1, iteration 700, loss = 1.6895\n",
      "Validation:\n",
      "     Got 483 / 1000 correct (48.30%)\n",
      "Epoch 2, iteration 0, loss = 1.2570\n",
      "Validation:\n",
      "     Got 482 / 1000 correct (48.20%)\n",
      "Epoch 2, iteration 100, loss = 1.4632\n",
      "Validation:\n",
      "     Got 511 / 1000 correct (51.10%)\n",
      "Epoch 2, iteration 200, loss = 1.1042\n",
      "Validation:\n",
      "     Got 507 / 1000 correct (50.70%)\n",
      "Epoch 2, iteration 300, loss = 1.3875\n",
      "Validation:\n",
      "     Got 474 / 1000 correct (47.40%)\n",
      "Epoch 2, iteration 400, loss = 1.2771\n",
      "Validation:\n",
      "     Got 481 / 1000 correct (48.10%)\n",
      "Epoch 2, iteration 500, loss = 1.4390\n",
      "Validation:\n",
      "     Got 492 / 1000 correct (49.20%)\n",
      "Epoch 2, iteration 600, loss = 1.5391\n",
      "Validation:\n",
      "     Got 481 / 1000 correct (48.10%)\n",
      "Epoch 2, iteration 700, loss = 1.5142\n",
      "Validation:\n",
      "     Got 510 / 1000 correct (51.00%)\n",
      "Epoch 3, iteration 0, loss = 1.1224\n",
      "Validation:\n",
      "     Got 493 / 1000 correct (49.30%)\n",
      "Epoch 3, iteration 100, loss = 1.3784\n",
      "Validation:\n",
      "     Got 523 / 1000 correct (52.30%)\n",
      "Epoch 3, iteration 200, loss = 1.0130\n",
      "Validation:\n",
      "     Got 522 / 1000 correct (52.20%)\n",
      "Epoch 3, iteration 300, loss = 1.2794\n",
      "Validation:\n",
      "     Got 488 / 1000 correct (48.80%)\n",
      "Epoch 3, iteration 400, loss = 1.1556\n",
      "Validation:\n",
      "     Got 499 / 1000 correct (49.90%)\n",
      "Epoch 3, iteration 500, loss = 1.3563\n",
      "Validation:\n",
      "     Got 506 / 1000 correct (50.60%)\n",
      "Epoch 3, iteration 600, loss = 1.4356\n",
      "Validation:\n",
      "     Got 499 / 1000 correct (49.90%)\n",
      "Epoch 3, iteration 700, loss = 1.4000\n",
      "Validation:\n",
      "     Got 513 / 1000 correct (51.30%)\n",
      "Epoch 4, iteration 0, loss = 1.0286\n",
      "Validation:\n",
      "     Got 503 / 1000 correct (50.30%)\n",
      "Epoch 4, iteration 100, loss = 1.2978\n",
      "Validation:\n",
      "     Got 529 / 1000 correct (52.90%)\n",
      "Epoch 4, iteration 200, loss = 0.9319\n",
      "Validation:\n",
      "     Got 531 / 1000 correct (53.10%)\n",
      "Epoch 4, iteration 300, loss = 1.1872\n",
      "Validation:\n",
      "     Got 491 / 1000 correct (49.10%)\n",
      "Epoch 4, iteration 400, loss = 1.0514\n",
      "Validation:\n",
      "     Got 497 / 1000 correct (49.70%)\n",
      "Epoch 4, iteration 500, loss = 1.2773\n",
      "Validation:\n",
      "     Got 512 / 1000 correct (51.20%)\n",
      "Epoch 4, iteration 600, loss = 1.3379\n",
      "Validation:\n",
      "     Got 515 / 1000 correct (51.50%)\n",
      "Epoch 4, iteration 700, loss = 1.3033\n",
      "Validation:\n",
      "     Got 519 / 1000 correct (51.90%)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def two_layer_fc_init():\n",
    "    \"\"\"\n",
    "    Initialize the weights of a two-layer network, for use with the\n",
    "    two_layer_network function defined above. \n",
    "    You can use the `create_matrix_with_kaiming_normal` helper!\n",
    "    \n",
    "    Inputs: None\n",
    "    \n",
    "    Returns: A list of:\n",
    "    - w1: TensorFlow tf.Variable giving the weights for the first layer\n",
    "    - w2: TensorFlow tf.Variable giving the weights for the second layer\n",
    "    \"\"\"\n",
    "    hidden_layer_size = 4000\n",
    "    w1 = tf.Variable(create_matrix_with_kaiming_normal((3 * 32 * 32, 4000)))\n",
    "    w2 = tf.Variable(create_matrix_with_kaiming_normal((4000, 10)))\n",
    "    return [w1, w2]\n",
    "\n",
    "learning_rate = 1e-2\n",
    "print('Train')\n",
    "trained_params = train_part2(two_layer_fc, two_layer_fc_init, learning_rate,5)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set - DO THIS ONLY ONCE\n",
    "Now that we've gotten a result that we're happy with, we test our final model on the test set. This would be the score we would achieve on a competition. Think about how this compares to your validation set accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "     Got 5018 / 10000 correct (50.18%)\n"
     ]
    }
   ],
   "source": [
    "print('Test')\n",
    "check_accuracy(test_dset, two_layer_fc, trained_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barebones TensorFlow: Train a three-layer ConvNet\n",
    "We will now use TensorFlow to train a three-layer ConvNet on CIFAR-10.\n",
    "\n",
    "You need to implement the `three_layer_convnet_init` function. Recall that the architecture of the network is:\n",
    "\n",
    "1. Convolutional layer (with bias) with 32 5x5 filters, with zero-padding 2\n",
    "2. ReLU\n",
    "3. Convolutional layer (with bias) with 16 3x3 filters, with zero-padding 1\n",
    "4. ReLU\n",
    "5. Fully-connected layer (with bias) to compute scores for 10 classes\n",
    "\n",
    "You don't need to do any hyperparameter tuning, but you should see validation accuracies above 43% after one epoch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, iteration 0, loss = 2.7575\n",
      "Validation:\n",
      "     Got 92 / 1000 correct (9.20%)\n",
      "Epoch 0, iteration 100, loss = 1.8620\n",
      "Validation:\n",
      "     Got 334 / 1000 correct (33.40%)\n",
      "Epoch 0, iteration 200, loss = 1.6552\n",
      "Validation:\n",
      "     Got 377 / 1000 correct (37.70%)\n",
      "Epoch 0, iteration 300, loss = 1.7509\n",
      "Validation:\n",
      "     Got 392 / 1000 correct (39.20%)\n",
      "Epoch 0, iteration 400, loss = 1.8294\n",
      "Validation:\n",
      "     Got 435 / 1000 correct (43.50%)\n",
      "Epoch 0, iteration 500, loss = 1.6855\n",
      "Validation:\n",
      "     Got 452 / 1000 correct (45.20%)\n",
      "Epoch 0, iteration 600, loss = 1.5638\n",
      "Validation:\n",
      "     Got 460 / 1000 correct (46.00%)\n",
      "Epoch 0, iteration 700, loss = 1.5617\n",
      "Validation:\n",
      "     Got 472 / 1000 correct (47.20%)\n",
      "Epoch 1, iteration 0, loss = 1.4862\n",
      "Validation:\n",
      "     Got 476 / 1000 correct (47.60%)\n",
      "Epoch 1, iteration 100, loss = 1.4069\n",
      "Validation:\n",
      "     Got 487 / 1000 correct (48.70%)\n",
      "Epoch 1, iteration 200, loss = 1.3393\n",
      "Validation:\n",
      "     Got 494 / 1000 correct (49.40%)\n",
      "Epoch 1, iteration 300, loss = 1.5392\n",
      "Validation:\n",
      "     Got 477 / 1000 correct (47.70%)\n",
      "Epoch 1, iteration 400, loss = 1.4568\n",
      "Validation:\n",
      "     Got 506 / 1000 correct (50.60%)\n",
      "Epoch 1, iteration 500, loss = 1.5691\n",
      "Validation:\n",
      "     Got 511 / 1000 correct (51.10%)\n",
      "Epoch 1, iteration 600, loss = 1.3797\n",
      "Validation:\n",
      "     Got 509 / 1000 correct (50.90%)\n",
      "Epoch 1, iteration 700, loss = 1.4571\n",
      "Validation:\n",
      "     Got 516 / 1000 correct (51.60%)\n",
      "Epoch 2, iteration 0, loss = 1.3202\n",
      "Validation:\n",
      "     Got 524 / 1000 correct (52.40%)\n",
      "Epoch 2, iteration 100, loss = 1.3225\n",
      "Validation:\n",
      "     Got 524 / 1000 correct (52.40%)\n",
      "Epoch 2, iteration 200, loss = 1.1997\n",
      "Validation:\n",
      "     Got 524 / 1000 correct (52.40%)\n",
      "Epoch 2, iteration 300, loss = 1.4039\n",
      "Validation:\n",
      "     Got 508 / 1000 correct (50.80%)\n",
      "Epoch 2, iteration 400, loss = 1.2866\n",
      "Validation:\n",
      "     Got 530 / 1000 correct (53.00%)\n",
      "Epoch 2, iteration 500, loss = 1.4845\n",
      "Validation:\n",
      "     Got 524 / 1000 correct (52.40%)\n",
      "Epoch 2, iteration 600, loss = 1.3060\n",
      "Validation:\n",
      "     Got 528 / 1000 correct (52.80%)\n",
      "Epoch 2, iteration 700, loss = 1.3819\n",
      "Validation:\n",
      "     Got 544 / 1000 correct (54.40%)\n",
      "Epoch 3, iteration 0, loss = 1.2253\n",
      "Validation:\n",
      "     Got 545 / 1000 correct (54.50%)\n",
      "Epoch 3, iteration 100, loss = 1.2527\n",
      "Validation:\n",
      "     Got 553 / 1000 correct (55.30%)\n",
      "Epoch 3, iteration 200, loss = 1.1141\n",
      "Validation:\n",
      "     Got 542 / 1000 correct (54.20%)\n",
      "Epoch 3, iteration 300, loss = 1.3075\n",
      "Validation:\n",
      "     Got 516 / 1000 correct (51.60%)\n",
      "Epoch 3, iteration 400, loss = 1.1959\n",
      "Validation:\n",
      "     Got 545 / 1000 correct (54.50%)\n",
      "Epoch 3, iteration 500, loss = 1.4086\n",
      "Validation:\n",
      "     Got 543 / 1000 correct (54.30%)\n",
      "Epoch 3, iteration 600, loss = 1.2414\n",
      "Validation:\n",
      "     Got 550 / 1000 correct (55.00%)\n",
      "Epoch 3, iteration 700, loss = 1.3110\n",
      "Validation:\n",
      "     Got 555 / 1000 correct (55.50%)\n",
      "Epoch 4, iteration 0, loss = 1.1580\n",
      "Validation:\n",
      "     Got 556 / 1000 correct (55.60%)\n",
      "Epoch 4, iteration 100, loss = 1.1920\n",
      "Validation:\n",
      "     Got 560 / 1000 correct (56.00%)\n",
      "Epoch 4, iteration 200, loss = 1.0426\n",
      "Validation:\n",
      "     Got 549 / 1000 correct (54.90%)\n",
      "Epoch 4, iteration 300, loss = 1.2264\n",
      "Validation:\n",
      "     Got 534 / 1000 correct (53.40%)\n",
      "Epoch 4, iteration 400, loss = 1.1282\n",
      "Validation:\n",
      "     Got 561 / 1000 correct (56.10%)\n",
      "Epoch 4, iteration 500, loss = 1.3442\n",
      "Validation:\n",
      "     Got 559 / 1000 correct (55.90%)\n",
      "Epoch 4, iteration 600, loss = 1.1805\n",
      "Validation:\n",
      "     Got 570 / 1000 correct (57.00%)\n",
      "Epoch 4, iteration 700, loss = 1.2458\n",
      "Validation:\n",
      "     Got 568 / 1000 correct (56.80%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(5, 5, 3, 32) dtype=float32, numpy=\n",
       " array([[[[ 9.80165452e-02, -2.37495899e-01, -2.62201965e-01, ...,\n",
       "           -5.13042808e-02,  6.34962246e-02,  1.15145287e-02],\n",
       "          [-1.86923549e-01,  4.84663129e-01, -6.22280799e-02, ...,\n",
       "            1.45596340e-01, -1.88629642e-01,  6.21296652e-02],\n",
       "          [-2.02022374e-01,  2.17552036e-01,  1.05069363e-02, ...,\n",
       "           -3.65007184e-02, -2.70799696e-02, -1.12263992e-01]],\n",
       " \n",
       "         [[-1.36665314e-01,  6.03546649e-02,  2.00411499e-01, ...,\n",
       "            2.42839426e-01,  1.21673129e-01,  3.45899403e-01],\n",
       "          [ 9.88312066e-03, -5.46165593e-02,  1.02684848e-01, ...,\n",
       "           -2.96277046e-01,  2.26291656e-01, -1.41157046e-01],\n",
       "          [ 2.22679198e-01,  1.01205908e-01, -1.75849557e-01, ...,\n",
       "           -7.64839277e-02,  3.33512932e-01,  2.65787598e-02]],\n",
       " \n",
       "         [[ 1.06435291e-01, -1.19461857e-01,  1.73119992e-01, ...,\n",
       "           -1.26988634e-01, -2.69028656e-02, -1.34179935e-01],\n",
       "          [ 1.87787965e-01,  1.34378105e-01, -2.10060686e-01, ...,\n",
       "           -2.60681808e-01, -1.39543220e-01,  1.34830996e-01],\n",
       "          [ 6.01937361e-02,  1.22600362e-01,  2.77335867e-02, ...,\n",
       "           -1.01497248e-01, -9.69929174e-02,  9.90257487e-02]],\n",
       " \n",
       "         [[-9.59816799e-02, -1.97656676e-01,  8.30878504e-03, ...,\n",
       "           -8.64710212e-02,  3.98876220e-01,  3.61622684e-03],\n",
       "          [-3.49969685e-01, -1.29662946e-01,  1.08818404e-01, ...,\n",
       "            1.67187139e-01,  3.67576718e-01, -1.10887133e-01],\n",
       "          [ 7.52526149e-02,  2.34333649e-01,  9.75394174e-02, ...,\n",
       "            7.57749528e-02,  1.14277445e-01, -5.76191954e-03]],\n",
       " \n",
       "         [[ 9.34511125e-02,  2.02219263e-01,  2.48118341e-01, ...,\n",
       "           -3.22620049e-02,  8.49339142e-02,  1.78598002e-01],\n",
       "          [ 1.23458572e-01, -1.85263261e-01,  5.81365526e-02, ...,\n",
       "           -1.50213569e-01,  4.37018834e-02, -4.94303703e-02],\n",
       "          [ 1.69618838e-02, -1.68966085e-01,  1.64736986e-01, ...,\n",
       "            1.99914470e-01, -3.60995680e-02, -6.65399898e-03]]],\n",
       " \n",
       " \n",
       "        [[[ 2.18423575e-01, -1.40043134e-02,  1.64308071e-01, ...,\n",
       "            1.31603584e-01,  6.39679730e-02, -1.93980724e-01],\n",
       "          [ 2.13411272e-01,  1.87830612e-01, -7.04600587e-02, ...,\n",
       "           -7.13484287e-02, -4.51971948e-01, -1.32489368e-01],\n",
       "          [-2.14754105e-01,  1.26562208e-01, -2.38338932e-01, ...,\n",
       "            2.28509799e-01,  8.49201828e-02,  6.98991418e-02]],\n",
       " \n",
       "         [[-2.44178809e-02, -8.41636285e-02, -1.30054161e-01, ...,\n",
       "           -5.17059378e-02,  1.32352218e-01,  1.89140275e-01],\n",
       "          [ 1.46353349e-01,  2.59354085e-01, -1.06002085e-01, ...,\n",
       "           -9.20320973e-02,  1.18426718e-01,  1.51251443e-02],\n",
       "          [ 7.65094310e-02, -2.36803174e-01, -1.47572607e-01, ...,\n",
       "            1.49843320e-01, -2.29351625e-01, -2.28876501e-01]],\n",
       " \n",
       "         [[ 2.37797182e-02,  7.94633254e-02,  3.97219732e-02, ...,\n",
       "            2.73276423e-03,  1.70405895e-01,  1.00821972e-01],\n",
       "          [ 1.74233783e-02,  2.21913412e-01, -6.47104830e-02, ...,\n",
       "            1.38376743e-01, -8.30616429e-02, -6.14092872e-02],\n",
       "          [-1.52508810e-01, -7.88835064e-02,  2.29906919e-03, ...,\n",
       "            2.29642302e-01, -9.89207923e-02, -2.33785689e-01]],\n",
       " \n",
       "         [[-1.04700051e-01,  5.05469516e-02, -1.31927520e-01, ...,\n",
       "           -3.39448333e-01,  9.93713364e-02,  1.57201275e-01],\n",
       "          [ 2.79506538e-02,  4.32607643e-02, -7.30303377e-02, ...,\n",
       "           -1.72490269e-01, -2.63561197e-02,  3.69750470e-01],\n",
       "          [ 4.19064835e-02, -2.24649861e-01, -1.69108063e-01, ...,\n",
       "            5.24783395e-02,  5.47328480e-02,  2.94787977e-02]],\n",
       " \n",
       "         [[-1.90463439e-02,  9.26997811e-02, -3.06607068e-01, ...,\n",
       "            2.68483222e-01, -8.32107943e-03,  3.25865671e-02],\n",
       "          [ 1.00651793e-01,  6.85050115e-02, -4.65030164e-01, ...,\n",
       "            3.62107903e-01, -1.46179974e-01,  2.46921748e-01],\n",
       "          [ 3.34500134e-01, -3.14234942e-02,  1.27722388e-02, ...,\n",
       "           -1.53055385e-01,  1.41255945e-01, -4.88713160e-02]]],\n",
       " \n",
       " \n",
       "        [[[-1.85875669e-01,  1.03010297e-01,  1.36909988e-02, ...,\n",
       "           -1.07965181e-02,  1.23457752e-01,  6.78548813e-02],\n",
       "          [ 1.42106228e-02, -1.09553896e-01, -2.74127722e-03, ...,\n",
       "           -1.39301687e-01,  6.77320138e-02,  9.75145400e-02],\n",
       "          [-2.60327160e-01, -3.64296585e-01,  2.63126105e-01, ...,\n",
       "            9.57584158e-02, -3.26232910e-02, -2.64084581e-02]],\n",
       " \n",
       "         [[ 5.77378236e-02, -2.52535520e-03,  3.05198282e-01, ...,\n",
       "            2.41448522e-01,  5.53916581e-02,  7.76666403e-02],\n",
       "          [-1.20520197e-01,  1.38049379e-01,  6.79630553e-04, ...,\n",
       "            2.97605842e-02,  1.09701484e-01, -3.08944672e-01],\n",
       "          [-1.35315791e-01,  2.11614296e-01,  6.61502257e-02, ...,\n",
       "            3.66349742e-02,  8.32011327e-02,  1.79825231e-01]],\n",
       " \n",
       "         [[ 1.65564939e-01,  4.12893109e-02,  3.76596972e-02, ...,\n",
       "           -3.22277367e-01,  1.90927744e-01,  8.74568596e-02],\n",
       "          [-2.76686728e-01, -5.94213232e-02, -1.56943072e-02, ...,\n",
       "            1.00911163e-01, -8.15859661e-02, -1.16157584e-01],\n",
       "          [ 1.65244907e-01, -1.22457534e-01,  7.34061897e-02, ...,\n",
       "            2.68239100e-02,  1.52081694e-03, -1.67822286e-01]],\n",
       " \n",
       "         [[ 1.52342498e-01, -2.35743642e-01, -3.26123297e-01, ...,\n",
       "           -3.01606923e-01,  2.17691958e-02, -1.96557522e-01],\n",
       "          [-2.53536016e-01, -5.13732061e-02,  2.27569160e-03, ...,\n",
       "            2.16458410e-01, -9.52192694e-02,  9.56094116e-02],\n",
       "          [-1.38195911e-02, -9.42336023e-02, -2.36164615e-01, ...,\n",
       "           -1.01501077e-01,  8.83350447e-02, -6.67748675e-02]],\n",
       " \n",
       "         [[ 2.52731621e-01, -2.40476251e-01,  2.51543391e-02, ...,\n",
       "            1.46516338e-01, -3.09160262e-01, -1.12208731e-01],\n",
       "          [ 2.50535663e-02, -2.23605096e-01,  7.05686882e-02, ...,\n",
       "            5.81420660e-02,  4.40918207e-02, -1.14915334e-01],\n",
       "          [-4.99100201e-02, -2.29881689e-01, -2.72689182e-02, ...,\n",
       "            1.43151404e-02,  2.12825090e-01, -1.14902779e-01]]],\n",
       " \n",
       " \n",
       "        [[[ 5.01198061e-02, -3.52504775e-02,  1.96573749e-01, ...,\n",
       "            2.26232293e-03, -2.98776835e-01,  2.69195765e-01],\n",
       "          [ 2.61619966e-02, -6.35806099e-02, -3.80418390e-01, ...,\n",
       "           -3.23120415e-01, -1.34147078e-01, -1.12773553e-01],\n",
       "          [ 4.13265564e-02, -1.21838309e-01, -3.98080677e-01, ...,\n",
       "           -9.29950401e-02, -1.27075776e-01, -2.30633453e-01]],\n",
       " \n",
       "         [[-2.07416326e-01,  2.23624744e-02, -2.90824138e-02, ...,\n",
       "            2.03820273e-01, -3.01592976e-01,  2.59762462e-02],\n",
       "          [ 2.98845042e-02,  6.21963665e-02, -1.50251640e-02, ...,\n",
       "           -3.95954810e-02,  3.35612185e-02, -1.70678020e-01],\n",
       "          [-5.16361557e-02,  2.75195450e-01, -1.50151253e-01, ...,\n",
       "            3.63649309e-01,  2.41098404e-02, -7.30209053e-02]],\n",
       " \n",
       "         [[ 1.85514539e-01, -7.99904689e-02,  4.24906701e-01, ...,\n",
       "            1.68926106e-03,  7.87464306e-02, -1.98404536e-01],\n",
       "          [ 7.50215426e-02,  5.30011505e-02, -7.01649860e-02, ...,\n",
       "           -4.12744194e-01,  1.11598693e-01,  1.16745134e-04],\n",
       "          [-4.55449298e-02,  2.56743759e-01, -2.97531355e-02, ...,\n",
       "           -1.64396111e-02,  5.06538935e-02,  9.45630744e-02]],\n",
       " \n",
       "         [[ 4.06385809e-02,  4.35320064e-02, -9.62704886e-03, ...,\n",
       "           -2.22663432e-01, -3.87266353e-02, -1.33060202e-01],\n",
       "          [ 1.56260692e-02,  3.66356909e-01, -3.77739131e-01, ...,\n",
       "           -1.46331400e-01,  1.02951899e-01, -1.13022052e-01],\n",
       "          [-1.61437720e-01, -2.30261356e-01,  2.04129905e-01, ...,\n",
       "            1.72889933e-01, -3.67924757e-02, -1.79333419e-01]],\n",
       " \n",
       "         [[ 4.37544324e-02,  2.23320439e-01, -1.46391436e-01, ...,\n",
       "            2.43038476e-01,  1.90623879e-01,  9.51828510e-02],\n",
       "          [-1.35673359e-01,  1.55254841e-01, -8.26113522e-02, ...,\n",
       "            3.09747428e-01, -2.73434799e-02, -1.91000804e-01],\n",
       "          [-7.41631761e-02, -3.17357332e-02,  1.37672082e-01, ...,\n",
       "            1.93831965e-01,  1.41498446e-01,  3.17444563e-01]]],\n",
       " \n",
       " \n",
       "        [[[ 7.67019913e-02, -5.40917180e-02,  3.75204869e-02, ...,\n",
       "           -1.70202479e-01,  1.80911511e-01, -2.13913977e-01],\n",
       "          [ 1.56509474e-01,  4.59133476e-01, -9.79318768e-02, ...,\n",
       "           -2.86932141e-01,  4.41537425e-02,  1.18880786e-01],\n",
       "          [ 1.94426909e-01, -2.25944747e-03,  1.60538957e-01, ...,\n",
       "            1.62526041e-01,  8.77490193e-02, -2.62257725e-01]],\n",
       " \n",
       "         [[ 2.09239064e-04, -2.26401016e-01, -1.98638663e-01, ...,\n",
       "           -1.85077593e-01,  3.13054770e-01, -4.58679050e-01],\n",
       "          [-1.58368468e-01,  2.30064895e-02, -4.06962425e-01, ...,\n",
       "            2.34185785e-01,  1.39600471e-01, -6.75819814e-02],\n",
       "          [-2.32682358e-02, -4.12481800e-02,  2.97035635e-01, ...,\n",
       "           -1.36864796e-01,  5.09644687e-01,  8.35022479e-02]],\n",
       " \n",
       "         [[ 2.28728831e-01,  6.27197027e-02, -1.82608053e-01, ...,\n",
       "            1.49805322e-01,  3.17019671e-02, -8.75899494e-02],\n",
       "          [-3.39689434e-01, -2.95065969e-01, -1.67669505e-01, ...,\n",
       "           -2.78806567e-01, -1.05378898e-02,  8.21903273e-02],\n",
       "          [-2.75929630e-01,  9.04141068e-02,  2.11030990e-01, ...,\n",
       "            1.29336253e-01, -6.73941299e-02,  1.42674252e-01]],\n",
       " \n",
       "         [[-1.10077020e-02, -2.50416011e-01,  1.78559363e-01, ...,\n",
       "           -5.60227297e-02,  7.27184713e-02, -2.22660005e-01],\n",
       "          [-1.05633326e-01, -2.17211440e-01,  2.93435842e-01, ...,\n",
       "            4.10855785e-02, -1.67775556e-01, -2.00613230e-01],\n",
       "          [ 6.21197633e-02,  2.82093525e-01, -1.62161347e-02, ...,\n",
       "            4.92347814e-02,  3.13350320e-01,  8.00704882e-02]],\n",
       " \n",
       "         [[-1.56421572e-01, -1.99686453e-01,  6.87009171e-02, ...,\n",
       "            5.39366938e-02,  2.20049322e-02, -3.13896202e-02],\n",
       "          [-1.04878023e-01, -3.37589048e-02,  1.05121627e-01, ...,\n",
       "            3.11153114e-01, -4.40006657e-03, -4.41239566e-01],\n",
       "          [ 7.48617500e-02,  8.79646540e-02,  3.35247442e-02, ...,\n",
       "           -5.11330031e-02,  1.21318862e-01, -7.34301060e-02]]]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(32,) dtype=float32, numpy=\n",
       " array([ 0.02631589, -0.00724457, -0.0902051 , -0.0796026 ,  0.02351242,\n",
       "         0.05922193, -0.0252225 , -0.00384012, -0.09966827,  0.059948  ,\n",
       "         0.0542358 ,  0.08446809, -0.0470443 ,  0.06255052, -0.0044681 ,\n",
       "        -0.04375821, -0.00806668,  0.11570282, -0.00955439, -0.0503845 ,\n",
       "         0.08255509,  0.04599589, -0.00965444,  0.0915451 ,  0.08874457,\n",
       "        -0.03347414,  0.05863509, -0.04138869, -0.03808474, -0.0317478 ,\n",
       "        -0.01997014,  0.01859617], dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(3, 3, 32, 16) dtype=float32, numpy=\n",
       " array([[[[ 0.07929825,  0.08089074,  0.08184035, ..., -0.03241041,\n",
       "           -0.00298807, -0.03162105],\n",
       "          [-0.15404898,  0.01010359, -0.0620219 , ..., -0.07219134,\n",
       "           -0.02074861, -0.13779825],\n",
       "          [-0.09364857, -0.01331938,  0.03730499, ..., -0.02697789,\n",
       "           -0.01781196,  0.01046042],\n",
       "          ...,\n",
       "          [ 0.03689507, -0.20967144,  0.06220938, ...,  0.08528965,\n",
       "            0.06326474, -0.04864495],\n",
       "          [ 0.06258419,  0.08191147,  0.04224594, ..., -0.12632477,\n",
       "            0.17125309, -0.00783139],\n",
       "          [-0.02358726, -0.0258217 , -0.03128943, ...,  0.06584093,\n",
       "           -0.05571853,  0.01196188]],\n",
       " \n",
       "         [[-0.01720395,  0.06761975,  0.19047615, ...,  0.2102799 ,\n",
       "           -0.02826989, -0.01821878],\n",
       "          [-0.06333487,  0.02925893, -0.09185606, ...,  0.04138675,\n",
       "            0.02378192, -0.10746646],\n",
       "          [-0.09734486, -0.04125788,  0.12507279, ..., -0.00758989,\n",
       "           -0.00444842, -0.05441565],\n",
       "          ...,\n",
       "          [ 0.12408157, -0.08167366,  0.04236533, ..., -0.05421632,\n",
       "            0.08340675, -0.00276538],\n",
       "          [-0.11248416, -0.02447759, -0.10147999, ...,  0.04934596,\n",
       "           -0.04070144, -0.01960031],\n",
       "          [-0.02210261, -0.00659134,  0.03795794, ...,  0.21487308,\n",
       "            0.06206744, -0.08283249]],\n",
       " \n",
       "         [[ 0.00315111, -0.07001028, -0.00878556, ...,  0.11769585,\n",
       "            0.05289199, -0.01787537],\n",
       "          [ 0.00502058,  0.07786859, -0.04225402, ..., -0.07393406,\n",
       "           -0.00666538,  0.03562858],\n",
       "          [-0.12802842,  0.10695498, -0.10755062, ..., -0.08218721,\n",
       "            0.00981587, -0.06354685],\n",
       "          ...,\n",
       "          [-0.13574132, -0.07122306, -0.05321401, ...,  0.05937656,\n",
       "            0.08083161,  0.04506391],\n",
       "          [-0.04565961, -0.02016886,  0.12317366, ..., -0.02784932,\n",
       "            0.07138761,  0.10407642],\n",
       "          [-0.01019041, -0.01755388, -0.04246284, ...,  0.0745206 ,\n",
       "            0.24059731, -0.08005139]]],\n",
       " \n",
       " \n",
       "        [[[ 0.01162272,  0.11414222,  0.06974476, ..., -0.08168267,\n",
       "           -0.04649043,  0.24831145],\n",
       "          [-0.13793756, -0.02287961,  0.00923741, ...,  0.08928279,\n",
       "           -0.26089594,  0.15167508],\n",
       "          [ 0.01077429, -0.09707189, -0.04582467, ...,  0.11470363,\n",
       "            0.09948445,  0.06175096],\n",
       "          ...,\n",
       "          [ 0.01910095,  0.05095077,  0.08644843, ...,  0.11934099,\n",
       "           -0.0362665 , -0.09956498],\n",
       "          [-0.00406475, -0.10131615, -0.0249094 , ..., -0.11704838,\n",
       "            0.25504622,  0.00730929],\n",
       "          [-0.07352658,  0.13071397, -0.12087593, ..., -0.15492088,\n",
       "           -0.06317651,  0.05060619]],\n",
       " \n",
       "         [[-0.14548302, -0.01218803,  0.02694165, ..., -0.0489785 ,\n",
       "            0.03330233,  0.17864987],\n",
       "          [ 0.00276254, -0.1332132 ,  0.003028  , ..., -0.19632836,\n",
       "            0.07584493, -0.12441105],\n",
       "          [-0.03928917, -0.05985859,  0.07652649, ...,  0.01372111,\n",
       "            0.05565198, -0.12466355],\n",
       "          ...,\n",
       "          [ 0.02254112, -0.06831049, -0.0490676 , ..., -0.00762201,\n",
       "            0.09561055, -0.12706612],\n",
       "          [ 0.00831447,  0.00901884,  0.03902587, ..., -0.02724575,\n",
       "           -0.09015916,  0.03831935],\n",
       "          [ 0.17286907,  0.15925534,  0.18160874, ..., -0.23312767,\n",
       "            0.02612471, -0.09928501]],\n",
       " \n",
       "         [[-0.08143915, -0.01960791, -0.00172974, ..., -0.07441719,\n",
       "           -0.1316817 , -0.15062183],\n",
       "          [-0.03910021,  0.00159237, -0.06701244, ...,  0.16273758,\n",
       "           -0.23908335,  0.06531923],\n",
       "          [-0.1885129 ,  0.00589456,  0.09517708, ...,  0.1682248 ,\n",
       "           -0.06841353, -0.02123976],\n",
       "          ...,\n",
       "          [ 0.07982983, -0.09737041, -0.1545174 , ...,  0.07275025,\n",
       "           -0.03066256,  0.22270189],\n",
       "          [-0.01921407, -0.0710894 , -0.06119387, ..., -0.07085198,\n",
       "            0.11631677,  0.108304  ],\n",
       "          [ 0.01724712,  0.04861875, -0.03140899, ...,  0.07853178,\n",
       "            0.01764528, -0.02313022]]],\n",
       " \n",
       " \n",
       "        [[[-0.00825664, -0.0150074 , -0.0401    , ..., -0.05196591,\n",
       "            0.01820554,  0.02132766],\n",
       "          [-0.17120896,  0.05475777,  0.12054361, ..., -0.19845244,\n",
       "            0.1921213 ,  0.03961235],\n",
       "          [ 0.01001556, -0.02688074,  0.04591168, ..., -0.15487571,\n",
       "            0.06038975,  0.01064883],\n",
       "          ...,\n",
       "          [-0.06109645, -0.06283204,  0.03307199, ..., -0.03280576,\n",
       "            0.02538799,  0.15559618],\n",
       "          [-0.09449767,  0.00608107,  0.05901196, ...,  0.07418051,\n",
       "           -0.22762506, -0.18802887],\n",
       "          [ 0.01865522, -0.0450589 , -0.03919718, ..., -0.05122911,\n",
       "            0.07312381,  0.00730324]],\n",
       " \n",
       "         [[-0.08106682, -0.03256849, -0.01112097, ...,  0.11503054,\n",
       "           -0.14822796,  0.1462048 ],\n",
       "          [ 0.01165291, -0.00527635, -0.15474302, ...,  0.02953975,\n",
       "            0.01660775, -0.00638166],\n",
       "          [-0.00901346,  0.01919044,  0.02162433, ..., -0.05953392,\n",
       "           -0.01736936, -0.18705988],\n",
       "          ...,\n",
       "          [-0.09951507, -0.02664466,  0.01036149, ..., -0.0680721 ,\n",
       "           -0.06660356,  0.12953593],\n",
       "          [ 0.03386727,  0.03943345, -0.00407475, ..., -0.02648328,\n",
       "           -0.04384761, -0.07997666],\n",
       "          [-0.05955297, -0.05567358,  0.07884052, ..., -0.16601063,\n",
       "           -0.04087745, -0.07496654]],\n",
       " \n",
       "         [[ 0.02112504, -0.02275903,  0.02746051, ...,  0.13473746,\n",
       "           -0.02625929, -0.06432208],\n",
       "          [-0.04108617,  0.08619732,  0.01058163, ..., -0.03188395,\n",
       "           -0.03385589, -0.11250902],\n",
       "          [-0.08641221, -0.09391985, -0.0165079 , ..., -0.01490115,\n",
       "            0.1174659 , -0.00624793],\n",
       "          ...,\n",
       "          [-0.06842023,  0.01441597,  0.03441208, ..., -0.00770655,\n",
       "           -0.20969062, -0.05652911],\n",
       "          [-0.0482449 , -0.04292836, -0.09675507, ..., -0.14528722,\n",
       "            0.01987597,  0.05795411],\n",
       "          [ 0.01535115, -0.07037312, -0.03078582, ...,  0.00743387,\n",
       "            0.02953176,  0.13643819]]]], dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(16,) dtype=float32, numpy=\n",
       " array([ 0.1085374 ,  0.23727362,  0.0940977 , -0.12744507,  0.13983054,\n",
       "        -0.03425282,  0.04709832, -0.02467296,  0.01826147,  0.02532187,\n",
       "        -0.00577632, -0.06192818, -0.06190085,  0.00132411, -0.0563462 ,\n",
       "         0.01113714], dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(16384, 10) dtype=float32, numpy=\n",
       " array([[ 6.39391458e-03,  1.03186388e-02, -1.04100816e-02, ...,\n",
       "          3.84166883e-03, -7.53789814e-03,  2.90798931e-03],\n",
       "        [ 2.59491266e-03,  8.54936708e-03,  1.19932275e-02, ...,\n",
       "         -1.47694796e-02, -1.12504130e-02, -1.02597401e-02],\n",
       "        [-2.29407530e-02,  8.50367919e-03, -1.80708673e-02, ...,\n",
       "          5.18449023e-03,  4.10390086e-03, -8.80989060e-03],\n",
       "        ...,\n",
       "        [ 1.97857264e-02, -2.43683662e-02,  5.94658405e-03, ...,\n",
       "          4.83866083e-03, -1.64232217e-02, -1.29685532e-02],\n",
       "        [-2.11415836e-03, -1.53584480e-02,  8.40273406e-03, ...,\n",
       "          2.30298587e-03, -1.22536728e-02, -3.15625337e-03],\n",
       "        [ 4.18660790e-03, -1.15534971e-02,  2.20501963e-02, ...,\n",
       "          2.08205947e-05, -1.83383524e-02, -9.57502518e-03]], dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(10,) dtype=float32, numpy=\n",
       " array([-0.01183204, -0.05606247,  0.03408738,  0.00731918,  0.05597197,\n",
       "        -0.01317317,  0.0264868 , -0.02490502,  0.02739995, -0.04529256],\n",
       "       dtype=float32)>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def three_layer_convnet_init():\n",
    "    \"\"\"\n",
    "    Initialize the weights of a Three-Layer ConvNet, for use with the\n",
    "    three_layer_convnet function defined above.\n",
    "    You can use the `create_matrix_with_kaiming_normal` helper!\n",
    "    \n",
    "    Inputs: None\n",
    "    \n",
    "    Returns a list containing:\n",
    "    - conv_w1: TensorFlow tf.Variable giving weights for the first conv layer\n",
    "    - conv_b1: TensorFlow tf.Variable giving biases for the first conv layer\n",
    "    - conv_w2: TensorFlow tf.Variable giving weights for the second conv layer\n",
    "    - conv_b2: TensorFlow tf.Variable giving biases for the second conv layer\n",
    "    - fc_w: TensorFlow tf.Variable giving weights for the fully-connected layer\n",
    "    - fc_b: TensorFlow tf.Variable giving biases for the fully-connected layer\n",
    "    \"\"\"\n",
    "    params = None\n",
    "    ############################################################################\n",
    "    # TODO: Initialize the parameters of the three-layer network.              #\n",
    "    ############################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    conv_w1 = tf.Variable(create_matrix_with_kaiming_normal((5, 5, 3, 32)))\n",
    "    conv_b1 = tf.Variable(tf.zeros((32,)))\n",
    "\n",
    "    conv_w2 = tf.Variable(create_matrix_with_kaiming_normal((3, 3, 32, 16)))\n",
    "    conv_b2 = tf.Variable(tf.zeros((16,)))\n",
    "\n",
    "    fc_w = tf.Variable(create_matrix_with_kaiming_normal((32 * 32 * 16, 10)))\n",
    "    fc_b = tf.Variable(tf.zeros((10,)))\n",
    "\n",
    "    params = [conv_w1, conv_b1, conv_w2, conv_b2, fc_w, fc_b]\n",
    "\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    ############################################################################\n",
    "    #                             END OF YOUR CODE                             #\n",
    "    ############################################################################\n",
    "    return params\n",
    "\n",
    "learning_rate = 3e-3\n",
    "train_part2(three_layer_convnet, three_layer_convnet_init, learning_rate,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part V: Train a _GREAT_ model on CIFAR-10!\n",
    "\n",
    "In this section you can experiment with whatever ConvNet architecture you'd like on CIFAR-10.\n",
    "\n",
    "You should experiment with architectures, hyperparameters, loss functions, regularization, or anything else you can think of to train a model that achieves **at least 70%** accuracy on the **validation** set within 10 epochs. You can use the built-in train function, the `train_part34` function from above, or implement your own training loop.\n",
    "\n",
    "Describe what you did at the end of the notebook.\n",
    "\n",
    "### Some things you can try:\n",
    "- **Filter size**: Above we used 5x5 and 3x3; is this optimal?\n",
    "- **Number of filters**: Above we used 16 and 32 filters. Would more or fewer do better?\n",
    "- **Pooling**: We didn't use any pooling above. Would this improve the model?\n",
    "- **Normalization**: Would your model be improved with batch normalization, layer normalization, group normalization, or some other normalization strategy?\n",
    "- **Network architecture**: The ConvNet above has only three layers of trainable parameters. Would a deeper model do better? Good architectures to try include:\n",
    "    - [conv-relu-pool]xN -> [affine]xM -> [softmax or SVM]\n",
    "    - [conv-relu-conv-relu-pool]xN -> [affine]xM -> [softmax or SVM]\n",
    "    - [batchnorm-relu-conv]xN -> [affine]xM -> [softmax or SVM]\n",
    "- **Global average pooling**: Instead of flattening after the final convolutional layer, would global average pooling do better? This strategy is used for example in Google's Inception network and in Residual Networks.\n",
    "- **Regularization**: Would some kind of regularization improve performance? Maybe weight decay or dropout?\n",
    "\n",
    "### NOTE: Batch Normalization / Dropout\n",
    "If you are using Batch Normalization and Dropout, remember to pass `is_training=True` if you use the `train_part34()` function. BatchNorm and Dropout layers have different behaviors at training and inference time. `training` is a specific keyword argument reserved for this purpose in any `tf.keras.Model`'s `call()` function. Read more about this here : https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/BatchNormalization#methods\n",
    "https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Dropout#methods\n",
    "\n",
    "### Tips for training\n",
    "For each network architecture that you try, you should tune the learning rate and other hyperparameters. When doing this there are a couple important things to keep in mind: \n",
    "\n",
    "- If the parameters are working well, you should see improvement within a few hundred iterations\n",
    "- Remember the coarse-to-fine approach for hyperparameter tuning: start by testing a large range of hyperparameters for just a few training iterations to find the combinations of parameters that are working at all.\n",
    "- Once you have found some sets of parameters that seem to work, search more finely around these parameters. You may need to train for more epochs.\n",
    "- You should use the validation set for hyperparameter search, and save your test set for evaluating your architecture on the best parameters as selected by the validation set.\n",
    "\n",
    "### Going above and beyond\n",
    "If you are feeling adventurous there are many other features you can implement to try and improve your performance. You are **not required** to implement any of these, but don't miss the fun if you have time!\n",
    "\n",
    "- Alternative optimizers: you can try Adam, Adagrad, RMSprop, etc.\n",
    "- Alternative activation functions such as leaky ReLU, parametric ReLU, ELU, or MaxOut.\n",
    "- Model ensembles\n",
    "- Data augmentation\n",
    "- New Architectures\n",
    "  - [ResNets](https://arxiv.org/abs/1512.03385) where the input from the previous layer is added to the output.\n",
    "  - [DenseNets](https://arxiv.org/abs/1608.06993) where inputs into previous layers are concatenated together.\n",
    "  - [This blog has an in-depth overview](https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32)\n",
    "  \n",
    "### Have fun and happy training! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_part34(model_init_fn, optimizer_init_fn, num_epochs=1, is_training=False):\n",
    "#  ############################################################################\n",
    "# # TODO: Train a model on CIFAR-10.                 #\n",
    "# ############################################################################\n",
    "# # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# pass\n",
    "\n",
    "# # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "# ############################################################################\n",
    "# #                            END OF YOUR CODE                              #\n",
    "# ############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import concatenate\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Functional API\n",
    "def miniGoogleNet(width, height, depth, classes):\n",
    "\tdef conv_module(x, K, kX, kY, stride, chanDim, padding=\"same\"):\n",
    "\t\t# define a CONV => BN => RELU pattern\n",
    "\t\tx = Conv2D(K, (kX, kY), strides=stride, padding=padding)(x)\n",
    "\t\tx = BatchNormalization(axis=chanDim)(x)\n",
    "\t\tx = Activation(\"relu\")(x)\n",
    "\t\treturn x\n",
    "\t\t\n",
    "\tdef inception_module(x, numK1x1, numK3x3, chanDim):\n",
    "\t\t# define two CONV modules, then concatenate across the channel dimension\n",
    "\t\tconv_1x1 = conv_module(x, numK1x1, 1, 1, (1, 1), chanDim)\n",
    "\t\tconv_3x3 = conv_module(x, numK3x3, 3, 3, (1, 1), chanDim)\n",
    "\t\tx = concatenate([conv_1x1, conv_3x3], axis=chanDim)\n",
    "\t\treturn x\n",
    "\t\t\n",
    "\tdef downsample_module(x, K, chanDim):\n",
    "\t\t# define the CONV module and POOL, then concatenate across the channel dimensions\n",
    "\t\tconv_3x3 = conv_module(x, K, 3, 3, (2, 2), chanDim, padding=\"valid\")\n",
    "\t\tpool = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\t\tx = concatenate([conv_3x3, pool], axis=chanDim)\n",
    "\t\treturn x\n",
    "\n",
    "    # initialize the input shape to be \"channels last\" and the channels dimension itself\n",
    "\tinputShape = (height, width, depth)\n",
    "\tchanDim = -1\n",
    "\n",
    "\t# define the model input and first CONV module\n",
    "\tinputs = Input(shape=inputShape)\n",
    "\tx = conv_module(inputs, 96, 3, 3, (1, 1), chanDim)\n",
    "\t\n",
    "    # two Inception modules followed by a downsample module\n",
    "\tx = inception_module(x, 32, 32, chanDim)\n",
    "\tx = inception_module(x, 32, 48, chanDim)\n",
    "\tx = downsample_module(x, 80, chanDim)\n",
    "\t\n",
    "    # four Inception modules followed by a downsample module\n",
    "\tx = inception_module(x, 112, 48, chanDim)\n",
    "\tx = inception_module(x, 96, 64, chanDim)\n",
    "\tx = inception_module(x, 80, 80, chanDim)\n",
    "\tx = inception_module(x, 48, 96, chanDim)\n",
    "\tx = downsample_module(x, 96, chanDim)\n",
    "\t\n",
    "    # two Inception modules followed by global POOL and dropout\n",
    "\tx = inception_module(x, 176, 160, chanDim)\n",
    "\tx = inception_module(x, 176, 160, chanDim)\n",
    "\tx = AveragePooling2D((7, 7))(x)\n",
    "\tx = Dropout(0.5)(x)\n",
    "\t\n",
    "    # softmax classifier\n",
    "\tx = Flatten()(x)\n",
    "\tx = Dense(classes)(x)\n",
    "\tx = Activation(\"softmax\")(x)\n",
    "\t\n",
    "    # create the model\n",
    "\tmodel = Model(inputs, x, name=\"minigooglenet\")\n",
    "\treturn model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomConvNet(tf.keras.Model):\n",
    "    def __init__(self, num_classes, channel_dim=-1):\n",
    "        super(CustomConvNet, self).__init__()\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "        # initialize the first CONV\n",
    "        self.conv_init = tf.keras.layers.Conv2D(96, (3, 3), strides=(1, 1), padding=\"same\")\n",
    "        \n",
    "        # initialize CONV_1x1_@ where @: no. of filters\n",
    "        self.conv_1x1_32 = tf.keras.layers.Conv2D(32, (1, 1), strides=(1, 1), padding=\"same\")\n",
    "        self.conv_1x1_96 = tf.keras.layers.Conv2D(96, (1, 1), strides=(1, 1), padding=\"same\")\n",
    "        self.conv_1x1_80 = tf.keras.layers.Conv2D(80, (1, 1), strides=(1, 1), padding=\"same\")\n",
    "        self.conv_1x1_48 = tf.keras.layers.Conv2D(48, (1, 1), strides=(1, 1), padding=\"same\")\n",
    "        self.conv_1x1_112 = tf.keras.layers.Conv2D(112, (1, 1), strides=(1, 1), padding=\"same\")\n",
    "        self.conv_1x1_176 = tf.keras.layers.Conv2D(176, (1, 1), strides=(1, 1), padding=\"same\")\n",
    "        \n",
    "        # initialize CONV_3x3_@ where @: no. of filters\n",
    "        self.conv_3x3_32 = tf.keras.layers.Conv2D(32, (3, 3), strides=(1, 1), padding=\"same\")\n",
    "        self.conv_3x3_48 = tf.keras.layers.Conv2D(48, (3, 3), strides=(1, 1), padding=\"same\")\n",
    "        self.conv_3x3_64 = tf.keras.layers.Conv2D(64, (3, 3), strides=(1, 1), padding=\"same\")\n",
    "        self.conv_3x3_80 = tf.keras.layers.Conv2D(80, (3, 3), strides=(1, 1), padding=\"same\")\n",
    "        self.conv_3x3_96 = tf.keras.layers.Conv2D(96, (3, 3), strides=(1, 1), padding=\"same\")\n",
    "        self.conv_3x3_160 = tf.keras.layers.Conv2D(160, (3, 3), strides=(1, 1), padding=\"same\")\n",
    "        \n",
    "        # initialize downsampling CONV_3x3 \n",
    "        self.conv_down_80 = tf.keras.layers.Conv2D(80, (3, 3), strides=(2, 2), padding=\"valid\")\n",
    "        self.conv_down_96 = tf.keras.layers.Conv2D(96, (3, 3), strides=(2, 2), padding=\"valid\")\n",
    "        \n",
    "        # initialize the downsampling POOL\n",
    "        self.maxpool = tf.keras.layers.MaxPooling2D((3,3), strides=(2,2))\n",
    "        \n",
    "        self.bn = tf.keras.layers.BatchNormalization(axis=channel_dim)\n",
    "        \n",
    "        self.relu = tf.keras.layers.Activation(\"relu\")\n",
    "        \n",
    "        self.avepool = tf.keras.layers.AveragePooling2D((7,7))\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(0.5)\n",
    "        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(num_classes)\n",
    "        \n",
    "        self.softmax = tf.keras.layers.Activation(\"softmax\")\n",
    "        \n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "    \n",
    "    def call(self, input_tensor, training=True):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "        x = self.conv_init(input_tensor)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # build 2 Inception modules followed by a downsample module\n",
    "        # 1st Inception module\n",
    "        x1 = self.conv_1x1_32(x)\n",
    "        x1 = self.bn(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x2 = self.conv_3x3_32(x)\n",
    "        x2 = self.bn(x2)\n",
    "        x2 = self.relu(x2)\n",
    "        x = tf.keras.layers.concatenate([x1, x2], axis=channel_dim)\n",
    "        # 2nd Inception module\n",
    "        x1 = self.conv_1x1_32(x)\n",
    "        x1 = self.bn(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x2 = self.conv_3x3_48(x)\n",
    "        x2 = self.bn(x2)\n",
    "        x2 = self.relu(x2)\n",
    "        x = tf.keras.layers.concatenate([x1, x2], axis=channel_dim)\n",
    "        # downsample module\n",
    "        x1 = self.conv_down_80(x)\n",
    "        x2 = self.maxpool(x)\n",
    "        x = tf.keras.layers.concatenate([x1, x2], axis=channel_dim)\n",
    "        \n",
    "        \n",
    "        # build 4 Inception modules followed by a downsample module\n",
    "        # 1st Inception module\n",
    "        x1 = self.conv_1x1_112(x)\n",
    "        x1 = self.bn(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x2 = self.conv_3x3_48(x)\n",
    "        x2 = self.bn(x2)\n",
    "        x2 = self.relu(x2)\n",
    "        x = tf.keras.layers.concatenate([x1, x2], axis=channel_dim)\n",
    "        # 2nd Inception module\n",
    "        x1 = self.conv_1x1_96(x)\n",
    "        x1 = self.bn(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x2 = self.conv_3x3_64(x)\n",
    "        x2 = self.bn(x2)\n",
    "        x2 = self.relu(x2)\n",
    "        x = tf.keras.layers.concatenate([x1, x2], axis=channel_dim)\n",
    "        # 3rd Inception module\n",
    "        x1 = self.conv_1x1_80(x)\n",
    "        x1 = self.bn(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x2 = self.conv_3x3_80(x)\n",
    "        x2 = self.bn(x2)\n",
    "        x2 = self.relu(x2)\n",
    "        x = tf.keras.layers.concatenate([x1, x2], axis=channel_dim)\n",
    "        # 4th Inception module\n",
    "        x1 = self.conv_1x1_48(x)\n",
    "        x1 = self.bn(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x2 = self.conv_3x3_96(x)\n",
    "        x2 = self.bn(x2)\n",
    "        x2 = self.relu(x2)\n",
    "        x = tf.keras.layers.concatenate([x1, x2], axis=channel_dim)\n",
    "        # downsample module\n",
    "        x1 = self.conv_down_96(x)\n",
    "        x2 = self.maxpool(x)\n",
    "        x = tf.keras.layers.concatenate([x1, x2], axis=channel_dim)\n",
    "        \n",
    "        \n",
    "        # build 2 Inception modules followed by global POOL and dropout\n",
    "        # 1st Inception module\n",
    "        x1 = self.conv_1x1_176(x)\n",
    "        x1 = self.bn(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x2 = self.conv_3x3_160(x)\n",
    "        x2 = self.bn(x2)\n",
    "        x2 = self.relu(x2)\n",
    "        x = tf.keras.layers.concatenate([x1, x2], axis=channel_dim)\n",
    "        # 2nd Inception module\n",
    "        x1 = self.conv_1x1_176(x)\n",
    "        x1 = self.bn(x1)\n",
    "        x1 = self.relu(x1)\n",
    "        x2 = self.conv_3x3_160(x)\n",
    "        x2 = self.bn(x2)\n",
    "        x2 = self.relu(x2)\n",
    "        x = tf.keras.layers.concatenate([x1, x2], axis=channel_dim)\n",
    "        # global pool\n",
    "        x = self.avepool(x)\n",
    "        # dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        \n",
    "        # build the softmax classifier\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading CIFAR-10 dataset...\n",
      "converting labels from integers to vectors...\n",
      "compiling model...\n",
      "training network...\n",
      "Epoch 1/10\n",
      "382/382 [==============================] - 626s 2s/step - loss: 1.7419 - accuracy: 0.3729 - val_loss: 1.4558 - val_accuracy: 0.5020\n",
      "Epoch 2/10\n",
      "382/382 [==============================] - 640s 2s/step - loss: 1.1306 - accuracy: 0.5983 - val_loss: 0.9049 - val_accuracy: 0.6790\n",
      "Epoch 3/10\n",
      "382/382 [==============================] - 647s 2s/step - loss: 0.9428 - accuracy: 0.6715 - val_loss: 2.2113 - val_accuracy: 0.5200\n",
      "Epoch 4/10\n",
      "382/382 [==============================] - 635s 2s/step - loss: 0.8116 - accuracy: 0.7190 - val_loss: 0.7321 - val_accuracy: 0.7630\n",
      "Epoch 5/10\n",
      "382/382 [==============================] - 601s 2s/step - loss: 0.7220 - accuracy: 0.7499 - val_loss: 0.8042 - val_accuracy: 0.7310\n",
      "Epoch 6/10\n",
      "382/382 [==============================] - 605s 2s/step - loss: 0.6479 - accuracy: 0.7781 - val_loss: 1.0643 - val_accuracy: 0.7070\n",
      "Epoch 7/10\n",
      "382/382 [==============================] - 637s 2s/step - loss: 0.5949 - accuracy: 0.7944 - val_loss: 0.6169 - val_accuracy: 0.8180\n",
      "Epoch 8/10\n",
      "382/382 [==============================] - 627s 2s/step - loss: 0.5634 - accuracy: 0.8085 - val_loss: 0.6668 - val_accuracy: 0.7940\n",
      "Epoch 9/10\n",
      "382/382 [==============================] - 632s 2s/step - loss: 0.5190 - accuracy: 0.8232 - val_loss: 0.7156 - val_accuracy: 0.7830\n",
      "Epoch 10/10\n",
      "382/382 [==============================] - 637s 2s/step - loss: 0.4899 - accuracy: 0.8327 - val_loss: 0.6268 - val_accuracy: 0.8110\n"
     ]
    }
   ],
   "source": [
    "#device = '/device:GPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "num_epochs = 10\n",
    "init_lr = 1e-3\n",
    "batch_size = 128\n",
    "\n",
    "# initialize the label names for the CIFAR-10 dataset\n",
    "labelNames = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \n",
    "              \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "# load the CIFAR-10 dataset\n",
    "print(\"loading CIFAR-10 dataset...\")\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()\n",
    "\n",
    "# convert the labels from integers to vectors\n",
    "print(\"converting labels from integers to vectors...\")\n",
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(y_train)\n",
    "y_val = lb.transform(y_val)\n",
    "y_test = lb.transform(y_test)\n",
    "\n",
    "# construct the image generator for data augmentation\n",
    "aug = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=18, \n",
    "                                                      zoom_range=0.15, \n",
    "                                                      width_shift_range=0.2, \n",
    "                                                      height_shift_range=0.2, \n",
    "                                                      shear_range=0.15, \n",
    "                                                      horizontal_flip=True, \n",
    "                                                      fill_mode=\"nearest\")\n",
    "\n",
    "# initialize the optimizer and compile the model\n",
    "print(\"compiling model...\")\n",
    "#model = CustomConvNet(len(labelNames))\n",
    "model = miniGoogleNet(32, 32, 3, len(labelNames))\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(lr=init_lr, decay=init_lr/num_epochs)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "# train the network\n",
    "print(\"training network...\")\n",
    "H = model.fit(aug.flow(X_train, y_train, batch_size=batch_size),\n",
    "              validation_data=(X_val, y_val),\n",
    "              steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "              epochs=num_epochs)\n",
    "\n",
    "# def model_init_fn():\n",
    "#     return CustomConvNet()\n",
    "\n",
    "# def optimizer_init_fn():\n",
    "#     learning_rate = 1e-3\n",
    "#     return tf.keras.optimizers.Adam(learning_rate) \n",
    "\n",
    "# print('Train')\n",
    "# trained_params =train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)\n",
    "\n",
    "# print('Test')\n",
    "# check_accuracy(test_dset, model_init_fn, trained_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating network...\n",
      "79/79 [==============================] - 21s 267ms/step - loss: 0.6702 - accuracy: 0.8011\n",
      "Test loss:  0.6701553463935852  Test accuracy:  0.8011000156402588\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    airplane       0.84      0.85      0.84      1000\n",
      "  automobile       0.82      0.98      0.89      1000\n",
      "        bird       0.65      0.85      0.73      1000\n",
      "         cat       0.75      0.65      0.70      1000\n",
      "        deer       0.94      0.53      0.68      1000\n",
      "         dog       0.85      0.68      0.76      1000\n",
      "        frog       0.62      0.95      0.75      1000\n",
      "       horse       0.91      0.81      0.85      1000\n",
      "        ship       0.95      0.86      0.90      1000\n",
      "       truck       0.94      0.85      0.89      1000\n",
      "\n",
      "    accuracy                           0.80     10000\n",
      "   macro avg       0.83      0.80      0.80     10000\n",
      "weighted avg       0.83      0.80      0.80     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the network\n",
    "print(\"evaluating network...\")\n",
    "scores = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "print(\"Test loss: \", scores[0], \" Test accuracy: \", scores[1])\n",
    "\n",
    "preds = model.predict(X_test, batch_size=batch_size)\n",
    "print(classification_report(y_test.argmax(axis=1),\n",
    "                            preds.argmax(axis=1), \n",
    "                            target_names=labelNames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = np.arange(0, num_epochs)\n",
    "title = \"Training Loss and Accuracy on CIFAR-10\"\n",
    "\n",
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(N, H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(N, H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(title)\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "## Describe what you did \n",
    "\n",
    "In the cell below you should write an explanation of what you did, any additional features that you implemented, and/or any graphs that you made in the process of training and evaluating your network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "TODO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
