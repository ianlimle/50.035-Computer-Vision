{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "breeding-recognition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.layers import AveragePooling2D \n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from modules import config\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import imutils\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# import dependencies for inference\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "periodic-testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(H, N, plotPath):\n",
    "    # construct a plot that plots and saves the training history\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "    plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "    plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "    plt.title(\"Training Loss and Accuracy\")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.savefig(plotPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-sister",
   "metadata": {},
   "outputs": [],
   "source": [
    "totalTrain = len(list(imutils.paths.list_images(config.TRAIN_PATH)))\n",
    "totalVal = len(list(imutils.paths.list_images(config.VAL_PATH)))\n",
    "totalTest = len(list(imutils.paths.list_images(config.TEST_PATH)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-andrews",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the image generator for data augmentation\n",
    "trainAug = ImageDataGenerator(rotation_range=18,\n",
    "                              zoom_range=0.15,\n",
    "                              width_shift_range=0.2, \n",
    "                              height_shift_range=0.2,\n",
    "                              shear_range=0.15, \n",
    "                              horizontal_flip=True, \n",
    "                              fill_mode=\"nearest\")\n",
    "valAug = ImageDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-greenhouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the ImageNet mean subtraction (in RGB order) and set the\n",
    "# the mean subtraction value for each of the data augmentation objects\n",
    "mean = np.array([123.68, 116.779, 103.939], dtype=\"float32\")\n",
    "trainAug.mean = mean\n",
    "valAug.mean = mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-separate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the training generator\n",
    "trainGen = trainAug.flow_from_directory(config.TRAIN_PATH,\n",
    "                                        class_mode=\"categorical\",\n",
    "                                        target_size=(224, 224),\n",
    "                                        color_mode=\"rgb\",\n",
    "                                        shuffle=True,\n",
    "                                        batch_size=config.BS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-underground",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the validation generator\n",
    "valGen = valAug.flow_from_directory(config.VAL_PATH,\n",
    "                                    class_mode=\"categorical\",\n",
    "                                    target_size=(224, 224),\n",
    "                                    color_mode=\"rgb\",\n",
    "                                    shuffle=False,\n",
    "                                    batch_size=config.BS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-boxing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the testing generator\n",
    "testGen = valAug.flow_from_directory(config.TEST_PATH,\n",
    "                                     class_mode=\"categorical\",\n",
    "                                     target_size=(224, 224),\n",
    "                                     color_mode=\"rgb\",\n",
    "                                     shuffle=False,\n",
    "                                     batch_size=config.BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct our base model\n",
    "print(\"[INFO] compiling base model...\")\n",
    "base_model = ResNet50(input_tensor=Input(shape=(config.WIDTH,config.HEIGHT,config.DEPTH)),\n",
    "                      include_top=False,\n",
    "                      weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-companion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the head of the model that will be placed on top of the base model\n",
    "head_model = base_model.output\n",
    "head_model = AveragePooling2D(pool_size=(5, 5))(head_model)\n",
    "head_model = Flatten(name=\"flatten\")(head_model)\n",
    "head_model = Dense(256, activation=\"relu\")(head_model)\n",
    "head_model = Dropout(0.5)(head_model)\n",
    "head_model = Dense(len(config.CLASSES), activation=\"softmax\")(head_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finnish-keyboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=base_model.input, outputs=head_model)\n",
    "print(model.summary())\n",
    "print(base_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over all layers in the base model and freeze them so they will\n",
    "# *not* be updated during the training process\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-blame",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = Adam(lr=config.INIT_LR, decay=config.INIT_LR / config.WARMUP_EPOCHS)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-tracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the head_model (top layers added on top of the MobileNetV2 base model) \n",
    "print(\"[INFO] training head model...\")\n",
    "H = model.fit(trainGen,\n",
    "              validation_data=valGen,\n",
    "              validation_steps=totalVal // config.BS,\n",
    "              steps_per_epoch=totalTrain // config.BS,\n",
    "              epochs=config.WARMUP_EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-dollar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the testing generator and then use our trained model to\n",
    "# make predictions on the data\n",
    "print(\"[INFO] evaluating network...\")\n",
    "testGen.reset()\n",
    "predIdxs = model.predict(testGen, steps=(totalTest // config.BS) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-awareness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each image in the testing set we need to find the index of the\n",
    "# label with corresponding largest predicted probability\n",
    "predIdxs = np.argmax(predIdxs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a classification report\n",
    "print(classification_report(testGen.classes, predIdxs, target_names=testGen.class_indices.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-secretary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training history\n",
    "plot_training(H, config.WARMUP_EPOCHS, config.WARMUP_PLOT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-collect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the data generators\n",
    "trainGen.reset()\n",
    "valGen.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-adventure",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"No. of layers in the base model: \", len(base_model.layers))\n",
    "# Fine-tune from this layer onwards\n",
    "train_from_layer = 100\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[train_from_layer:]:\n",
    "  layer.trainable = True\n",
    "\n",
    "# show which layers are trainable\n",
    "for layer in base_model.layers:\n",
    "    print(\"{}: {}\".format(layer, layer.trainable))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-hughes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the changes to the model to take affect we need to recompile the model\n",
    "print(\"[INFO] re-compiling model...\")\n",
    "opt = Adam(lr=config.INIT_LR, decay=config.INIT_LR / config.FINETUNE_EPOCHS)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model again, this time fine-tuning *both* the final set\n",
    "# of CONV layers along with our set of FC layers\n",
    "H = model.fit(trainGen,\n",
    "              steps_per_epoch=totalTrain // config.BS,\n",
    "              validation_data=valGen,\n",
    "              validation_steps=totalVal // config.BS,\n",
    "              epochs=config.FINETUNE_EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-grove",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the testing generator and then use our trained model to\n",
    "# make predictions on the data\n",
    "print(\"[INFO] evaluating after fine-tuning network...\")\n",
    "testGen.reset()\n",
    "\n",
    "predIdxs = model.predict(testGen, steps=(totalTest // config.BS) + 1)\n",
    "predIdxs = np.argmax(predIdxs, axis=1)\n",
    "print(classification_report(testGen.classes, predIdxs, target_names=testGen.class_indices.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-perception",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(H, config.FINETUNE_EPOCHS, config.UNFROZEN_PLOT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-education",
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize the model to disk\n",
    "print(\"[INFO] serializing network...\")\n",
    "model.save(config.MODEL_PATH, save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-encounter",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the input image and then clone it so we can draw on it later\n",
    "image = cv2.imread(args[\"image\"])\n",
    "output = image.copy()\n",
    "output = imutils.resize(output, width=400)\n",
    "\n",
    "# our model was trained on RGB ordered images but OpenCV represents\n",
    "# images in BGR order, so swap the channels, and then resize to\n",
    "# 224x224 (the input dimensions for ResNet50)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image = cv2.resize(image, (224, 224))\n",
    "\n",
    "# convert the image to a floating point data type and perform mean\n",
    "# subtraction\n",
    "image = image.astype(\"float32\")\n",
    "mean = np.array([123.68, 116.779, 103.939][::-1], dtype=\"float32\")\n",
    "image -= mean\n",
    "\n",
    "# load the trained model from disk\n",
    "print(\"[INFO] loading model...\")\n",
    "model = load_model(config.MODEL_PATH)\n",
    "# pass the image through the network to obtain our predictions\n",
    "preds = model.predict(np.expand_dims(image, axis=0))[0]\n",
    "i = np.argmax(preds)\n",
    "label = config.CLASSES[i]\n",
    "\n",
    "# draw the prediction on the output image\n",
    "text = \"{}: {:.2f}%\".format(label, preds[i] * 100)\n",
    "cv2.putText(output, text, (3, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5,(0, 255, 0), 2)\n",
    "\n",
    "# show the output image\n",
    "cv2.imshow(\"Output\", output)\n",
    "cv2.waitKey(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
